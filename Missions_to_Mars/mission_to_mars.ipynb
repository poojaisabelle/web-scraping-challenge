{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "\n",
    "from splinter import Browser \n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 84.0.4147\n",
      "[WDM] - Get LATEST driver version for 84.0.4147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Driver [C:\\Users\\pooja\\.wdm\\drivers\\chromedriver\\win32\\84.0.4147.30\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Open the Chrome Driver Browser\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraping NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page URL that we are going to scrape\n",
    "news_url = \"https://mars.nasa.gov/news/\"\n",
    "browser.visit(news_url)\n",
    "\n",
    "# Convention to let the script we are running sleep to allow everything to load \n",
    "sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BeautifulSoup object and parse as lxml\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"NASA Engineers Checking InSight's Weather Sensors\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the latest news title \n",
    "latest_news_titles = soup.find_all(\"div\", class_=\"content_title\")\n",
    "news_title = latest_news_titles[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An electronics issue is suspected to be preventing the sensors from sharing their data about Mars weather with the spacecraft.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the paragraph (description) attached to the title above \n",
    "latest_news_paragraphs = soup.find_all(\"div\", class_=\"article_teaser_body\")\n",
    "news_paragraph = latest_news_paragraphs[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scraping JPL Mars Space Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page URL that we are going to scrape\n",
    "images_url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "browser.visit(images_url)\n",
    "\n",
    "# Convention to let the script we are running sleep to allow everything to load \n",
    "sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interact with the 'FULL IMAGE' button in the browser to get to the featured image\n",
    "browser.click_link_by_id('full_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interact with the 'more info' button in the browser to access the larger image size \n",
    "browser.click_link_by_partial_text('more info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BeautifulSoup object and parse as lxml\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the specific path to the larger image size \n",
    "lg_image_url = soup.find(class_=\"main_image\")[\"src\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the base url and path to image \n",
    "base_url = \"https://www.jpl.nasa.gov\"\n",
    "featured_image_url = base_url + lg_image_url\n",
    "featured_image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scrape Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page URL that we are going to scrape\n",
    "facts_url = \"https://space-facts.com/mars/\"\n",
    "\n",
    "# use the read_html function in Pandas to obtain the data on the page in HTML \n",
    "tables = pd.read_html(facts_url)\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the table that contains info on: Diameter, Mass, etc.\n",
    "df = tables[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df.columns = ['Description', 'Mars']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the Description as index\n",
    "df.set_index('Description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframe above to an HTML string\n",
    "html_table = df.to_html()\n",
    "print(html_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the html table \n",
    "df.to_html('table.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mars Hemisphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page URL that we are going to scrape\n",
    "hemisphere_url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(hemisphere_url)\n",
    "\n",
    "# Convention to let the script we are running sleep to allow everything to load \n",
    "sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BeautifulSoup object and parse as lxml\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the dictionaries (for each hemisphere)\n",
    "hemisphere_image_urls = list()\n",
    "\n",
    "# Retrieve all the hemisphere link titles\n",
    "link_titles = soup.find_all('h3')\n",
    "for title in link_titles:\n",
    "    print(title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the link titles \n",
    "for title in link_titles:\n",
    "    \n",
    "    # Click the next link that has the next title in link_titles\n",
    "    browser.click_link_by_partial_text(title.text)\n",
    "    sleep(0.5)\n",
    "    \n",
    "     # HTML object\n",
    "    html = browser.html\n",
    "    \n",
    "    # Parse HTML with Beautiful Soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    \n",
    "    # Retrieve the specific hemisphere name from the link titles \n",
    "    hem_name = title.text.split('Enhanced')[0]\n",
    "    \n",
    "    # Retrieve the full resolution image URL \n",
    "    img_path = soup.find(\"li\")\n",
    "    lg_img_url = img_path.a['href']\n",
    "    \n",
    "    # Append each dictionary to the list \n",
    "    hemisphere_image_urls.append({\n",
    "        \"title\": hem_name,\n",
    "        \"img_url\": lg_img_url\n",
    "    })\n",
    "    \n",
    "    # Go back to the home page before loop repeats\n",
    "    browser.visit(hemisphere_url)\n",
    "    sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check the list of dictionaries\n",
    "hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quit the browser \n",
    "broswer.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
